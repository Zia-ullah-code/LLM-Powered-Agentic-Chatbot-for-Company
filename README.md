LLM Powered Agentic Chatbot for Hexaa AI
ğŸ“Œ Project Overview

This project is a full-stack AI-powered chatbot system designed to assist potential clients in a professional, customer-service style. It interactively gathers project requirements, predicts project size, provides estimated costs, and restricts responses strictly to company services using Retrieval-Augmented Generation (RAG).

The chatbot can detect unhappy users and escalate to a human meeting booking flow. It maintains conversation memory, supports source citations, and includes a React-based frontend widget with an admin panel for knowledge base re-indexing.

âœ¨ Features

Conversational AI â€” Professional, context-aware chat.

RAG-powered answers â€” Only responds based on company-provided documents.

Project size prediction â€” Classifies into small, normal, large.

Cost estimation â€” Returns cost ranges depending on project size.

Sentiment detection â€” Escalates to human booking flow if user is unhappy.

Knowledge base re-indexing â€” Admin panel with one-click "Re-index".

Conversation memory â€” Stores last 20 messages in Redis.

Source citation â€” Provides document references where possible.

Multi-service architecture â€” Scalable deployment on Render or Vercel.

ğŸ› ï¸ Tech Stack

Frontend

React (chatbot widget + admin panel)

CSS-in-JS / inline styles

Backend

n8n â€” Workflow orchestration

ChromaDB â€” Vector database for knowledge base

Redis â€” Conversation memory

Embedder Service â€” Sentence-transformers for embeddings

Infrastructure

Docker â€” Multi-container local development


ğŸ—ï¸ System Architecture
[React Frontend]  <---->  [n8n Backend API]
                              |
                              |---(Redis) Conversation Memory
                              |---(Embedder) Create Text Embeddings
                              |---(Chroma) Retrieve Context Docs
                              |---(Sentiment Node) Detect Unhappiness
                              |---(Calendly) Escalation for Meetings

ğŸ”§ Service Breakdown
React Frontend

Floating chatbot widget

Markdown-formatted message display

Feedback buttons (ğŸ‘ ğŸ‘)

Admin panel with "Re-index" button

n8n Workflows

/predict â€” Detects intent, classifies request

/answer â€” Retrieves docs â†’ LLM â†’ formatted response with sources

/estimate â€” Predicts project size & cost range

/feedback â€” Stores user feedback

/reset â€” Clears conversation memory

/admin â€” Triggers knowledge base re-indexing

Redis

Stores chat history (conversation_id â†’ JSON array of messages)

Chroma

Stores vector embeddings for knowledge base

Used in RAG pipeline

Embedder

Converts company documents into embeddings for Chroma

âš™ï¸ How It Works

User opens chat â†’ React widget loads.

User sends message â†’ React sends request to /predict (n8n webhook).

n8n processes intent:

Negative sentiment / "request_human" â†’ Empathetic reply + Calendly link.

General question â†’ Retrieve docs from Chroma â†’ Call LLM â†’ Respond with sources.

Estimate request â†’ Ask follow-ups â†’ Predict size (small/normal/large) â†’ Return cost range.

Conversation is stored in Redis (max 20 messages).

React updates UI with professional formatting.

ğŸš€ Running Locally
1. Clone Repository
git clone https://github.com/Zia-ullah-code/LLM-Powered-Agentic-Chatbot-for-Company.git
cd chatbot_project

# This .env is part is not used for now
#2. Create .env file
#N8N_BASIC_AUTH_ACTIVE=true
#N8N_BASIC_AUTH_USER=admin
#N8N_BASIC_AUTH_PASSWORD=strongpassword
#N8N_ENCRYPTION_KEY=your_32_byte_random_key

3. Start with Docker
docker-compose up --build

4. Access Services

React (Frontend) â†’ http://localhost:3000

n8n (Backend) â†’ http://localhost:5678

Chroma â†’ http://localhost:8000

Redis â†’ localhost:6379
